import torch


def sample(
    likelihoods: torch.Tensor,
    mask: torch.Tensor,
    num_samples: int,
    sample_size: int,
    epsilon: float
) -> tuple[torch.Tensor, torch.Tensor]:
    """Generate `num_samples` samples, each consisting of `sample_size` items.
    Samples are generated by sampling uniformly with probability `epsilon`
    and sampling based on `likelihoods` with probability `1 - epsilon`.
    """
    sample_size = min(sample_size, mask.sum(dim=1).min().item())

    indices = []
    logps = []

    for _ in range(num_samples):
        sample_indices, sample_logp = _sample_without_replacement(likelihoods, mask, sample_size, epsilon)
        indices.append(sample_indices)
        logps.append(sample_logp)

    indices = torch.stack(indices, dim=1)
    logps = torch.stack(logps, dim=1)
    return indices, logps


def _sample_without_replacement(
    likelihoods: torch.Tensor,
    mask: torch.Tensor,
    sample_size: int,
    epsilon: float
) -> tuple[torch.Tensor, torch.Tensor]:
    """Generate a sample of size `sample_size`.
    Sample items are generated by sampling uniformly with probability `epsilon`
    and sampling based on `likelihoods` with probability `1 - epsilon`.
    """
    indices = torch.full_like(likelihoods[:, :sample_size], fill_value=-1, dtype=torch.long)
    logps = torch.zeros_like(likelihoods[:, :sample_size], dtype=torch.float)

    # Create a copy of the mask to avoid in-place operations
    mask = mask.clone()
    batch_indices = torch.arange(likelihoods.shape[0], device=likelihoods.device).unsqueeze(dim=1)

    for i in range(sample_size):
        # Sample uniformly
        uniform_indices = torch.multinomial(mask.float(), num_samples=1)
        uniform_probs = 1. / mask.sum(dim=1, keepdim=True)

        # Sample based on `likelihoods`
        probs = likelihoods / likelihoods.sum(dim=1, keepdim=True)
        likelihood_indices = torch.multinomial(probs, num_samples=1)
        likelihood_probs = probs.gather(dim=1, index=likelihood_indices)

        # Sample uniformly with a probability of `epsilon`, otherwise sample based on `likelihoods`
        is_uniform = (torch.rand_like(batch_indices.float()) < epsilon)
        col_i_indices = torch.where(is_uniform, uniform_indices, likelihood_indices)
        col_i_probs = torch.where(is_uniform, uniform_probs, likelihood_probs)

        indices[:, i] = col_i_indices.squeeze(dim=1)
        logps[:, i] = torch.log(col_i_probs).squeeze(dim=1)

        mask[batch_indices, col_i_indices] = 0
        likelihoods = likelihoods.masked_fill(~mask, 0.)

    logp = logps.sum(dim=1)
    return indices, logp


def compute_loss(logps: torch.Tensor, rewards: torch.Tensor, loss: str) -> torch.Tensor:
    """Compute REINFORCE loss with baseline."""
    if loss == 'reinforce':
        loss = -torch.mean(logps * rewards)
    elif loss == 'baseline':
        mean = rewards.mean(dim=1, keepdim=True)
        std = rewards.std(dim=1, keepdim=True)
        loss = -torch.mean(logps * (rewards - mean) / (std + 1e-9))

    return loss
