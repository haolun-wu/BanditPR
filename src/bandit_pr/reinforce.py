import torch


def sample(
    likelihoods: torch.Tensor,
    mask: torch.Tensor,
    num_samples: int,
    sample_size: int,
    epsilon: float
) -> tuple[torch.Tensor, torch.Tensor]:
    """Generates `num_samples` samples, each consisting of `sample_size` items.
    Samples are generated by sampling uniformly with probability `epsilon` and
    sampling based on `likelihoods` with probability `1 - epsilon`.
    """
    sample_size = min(sample_size, mask.sum(dim=1).min().item())

    indices = []
    log_probs = []

    for _ in range(num_samples):
        sample_indices, sample_log_prob = _sample_without_replacement(likelihoods, mask, sample_size, epsilon)
        indices.append(sample_indices)
        log_probs.append(sample_log_prob)

    indices = torch.stack(indices, dim=1)
    log_probs = torch.stack(log_probs, dim=1)
    return indices, log_probs


def _sample_without_replacement(
    likelihoods: torch.Tensor,
    mask: torch.Tensor,
    sample_size: int,
    epsilon: float
) -> tuple[torch.Tensor, torch.Tensor]:
    """Generates a sample of size `sample_size`.
    Sample items are generated by sampling uniformly with probability `epsilon` and
    sampling based on `likelihoods` with probability `1 - epsilon`.
    """
    indices = torch.full_like(likelihoods[:, :sample_size], fill_value=-1, dtype=torch.long)
    log_probs = torch.zeros_like(likelihoods[:, :sample_size], dtype=torch.float)

    # Creates a copy of the mask to avoid in-place operations
    mask = mask.clone()
    batch_indices = torch.arange(likelihoods.size(dim=0)).unsqueeze(dim=1)

    for i in range(sample_size):
        # Samples uniformly
        uniform_indices = torch.multinomial(mask.float(), num_samples=1)
        uniform_probs = 1. / mask.sum(dim=1, keepdim=True)

        # Samples based on `likelihoods`
        probs = likelihoods / likelihoods.sum(dim=1, keepdim=True)
        likelihood_indices = torch.multinomial(probs, num_samples=1)
        likelihood_probs = probs.gather(dim=1, index=likelihood_indices)

        # Samples uniformly with a probability of `epsilon`, otherwise samples based on `likelihoods`
        is_uniform = (torch.rand_like(batch_indices.float()) < epsilon)
        col_i_indices = torch.where(is_uniform, uniform_indices, likelihood_indices)
        col_i_probs = torch.where(is_uniform, uniform_probs, likelihood_probs)

        indices[:, i] = col_i_indices.squeeze(dim=1)
        log_probs[:, i] = torch.log(col_i_probs).squeeze(dim=1)

        mask[batch_indices, col_i_indices] = 0
        likelihoods = likelihoods.masked_fill(~mask, 0.)

    log_prob = log_probs.sum(dim=1)
    return indices, log_prob


def compute_loss(log_probs: torch.Tensor, rewards: torch.Tensor) -> torch.Tensor:
    """Computes REINFORCE loss."""
    return -torch.mean(log_probs * rewards)
