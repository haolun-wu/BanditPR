import logging

import torch


logger = logging.getLogger(__name__)


def sample(
    likelihoods: torch.Tensor,
    mask: torch.Tensor,
    num_samples: int,
    sample_size: int,
    epsilon: float
) -> tuple[torch.Tensor, torch.Tensor]:
    """Generates `num_samples` samples, each consisting of `sample_size` items.
    Samples are generated by sampling uniformly with probability `epsilon` and
    sampling based on `likelihoods` with probability `1 - epsilon`.
    """
    min_num_items = mask.sum(dim=1).min().item()

    if sample_size > min_num_items:
        logger.warning(
            f'sample_size ({sample_size}) is greater than '
            f'the minimum number of items ({min_num_items})'
        )
        sample_size = min_num_items

    indices = []
    log_probs = []

    for _ in range(num_samples):
        sample_indices, sample_log_prob = _sample_without_replacement(likelihoods, mask, sample_size, epsilon)
        indices.append(sample_indices)
        log_probs.append(sample_log_prob)

    indices = torch.stack(indices, dim=1)
    log_probs = torch.stack(log_probs, dim=1)
    return indices, log_probs


def _sample_without_replacement(
    likelihoods: torch.Tensor,
    mask: torch.Tensor,
    sample_size: int,
    epsilon: float
) -> tuple[torch.Tensor, torch.Tensor]:
    """Generates a sample of size `sample_size` without replacement.
    Samples are generated by sampling uniformly with probability `epsilon` and
    sampling based on `likelihoods` with probability `1 - epsilon`.
    """
    indices = torch.full_like(likelihoods[:, :sample_size], fill_value=-1, dtype=torch.long)
    log_probs = torch.zeros_like(likelihoods[:, :sample_size], dtype=torch.float)

    mask = mask.clone()
    batch_size = likelihoods.size(dim=0)
    batch_indices = torch.arange(batch_size).unsqueeze(dim=1)

    for i in range(sample_size):
        # Sample uniformly
        uniform_indices = torch.multinomial(mask.float(), num_samples=1)
        uniform_probs = 1. / mask.sum(dim=1, keepdim=True)

        # Sample based on likelihoods
        probs = likelihoods / likelihoods.sum(dim=1, keepdim=True)
        likelihood_indices = torch.multinomial(probs, num_samples=1)
        likelihood_probs = probs.gather(dim=1, index=likelihood_indices)

        # Sample uniformly with a probability of epsilon, otherwise sample based on likelihoods
        is_uniform = (torch.rand_like(uniform_indices.float()) < epsilon)
        col_i_indices = torch.where(is_uniform, uniform_indices, likelihood_indices)
        col_i_probs = torch.where(is_uniform, uniform_probs, likelihood_probs)

        indices[:, i] = col_i_indices.squeeze(dim=1)
        log_probs[:, i] = torch.log(col_i_probs).squeeze(dim=1)

        mask[batch_indices, col_i_indices] = 0
        likelihoods = likelihoods.masked_fill(~mask, 0.)

    log_prob = log_probs.sum(dim=1)
    return indices, log_prob


def compute_loss(log_probs: torch.Tensor, rewards: torch.Tensor) -> torch.Tensor:
    """Computes the REINFORCE loss with an average baseline."""
    # TODO: Consider using a moving average baseline
    baseline = torch.mean(rewards)
    loss = torch.mean(-(log_probs * ((rewards - baseline) / (baseline + 1e-9))))
    return loss
